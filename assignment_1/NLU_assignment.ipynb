{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y7dX0rKYH6Wk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24d3e1b8-8c6d-4a07-f563-12d9522adb65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-lg==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.7.1/en_core_web_lg-3.7.1-py3-none-any.whl (587.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.7/587.7 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-lg==3.7.1) (3.7.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.9.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (4.66.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.6.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (24.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.25.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.16.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.1.5)\n",
            "Installing collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-3.7.1\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "--2024-04-11 17:13:17--  https://raw.githubusercontent.com/BrownFortress/NLU-2024-Labs/main/labs/dataset/PennTreeBank/ptb.test.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 449945 (439K) [text/plain]\n",
            "Saving to: ‘dataset/PennTreeBank/ptb.test.txt’\n",
            "\n",
            "ptb.test.txt        100%[===================>] 439.40K  --.-KB/s    in 0.005s  \n",
            "\n",
            "2024-04-11 17:13:17 (92.5 MB/s) - ‘dataset/PennTreeBank/ptb.test.txt’ saved [449945/449945]\n",
            "\n",
            "--2024-04-11 17:13:17--  https://raw.githubusercontent.com/BrownFortress/NLU-2024-Labs/main/labs/dataset/PennTreeBank/ptb.valid.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 399782 (390K) [text/plain]\n",
            "Saving to: ‘dataset/PennTreeBank/ptb.valid.txt’\n",
            "\n",
            "ptb.valid.txt       100%[===================>] 390.41K  --.-KB/s    in 0.004s  \n",
            "\n",
            "2024-04-11 17:13:17 (94.5 MB/s) - ‘dataset/PennTreeBank/ptb.valid.txt’ saved [399782/399782]\n",
            "\n",
            "--2024-04-11 17:13:17--  https://raw.githubusercontent.com/BrownFortress/NLU-2024-Labs/main/labs/dataset/PennTreeBank/ptb.train.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5101618 (4.9M) [text/plain]\n",
            "Saving to: ‘dataset/PennTreeBank/ptb.train.txt’\n",
            "\n",
            "ptb.train.txt       100%[===================>]   4.87M  --.-KB/s    in 0.03s   \n",
            "\n",
            "2024-04-11 17:13:17 (187 MB/s) - ‘dataset/PennTreeBank/ptb.train.txt’ saved [5101618/5101618]\n",
            "\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.16.6-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-1.45.0-py2.py3-none-any.whl (267 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.1/267.1 kB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.43 docker-pycreds-0.4.0 gitdb-4.0.11 sentry-sdk-1.45.0 setproctitle-1.3.3 smmap-5.0.1 wandb-0.16.6\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "  import google.colab\n",
        "  IN_COLAB = True\n",
        "except:\n",
        "  IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "  !python -m spacy download en_core_web_lg\n",
        "  !wget -P dataset/PennTreeBank https://raw.githubusercontent.com/BrownFortress/NLU-2024-Labs/main/labs/dataset/PennTreeBank/ptb.test.txt\n",
        "  !wget -P dataset/PennTreeBank https://raw.githubusercontent.com/BrownFortress/NLU-2024-Labs/main/labs/dataset/PennTreeBank/ptb.valid.txt\n",
        "  !wget -P dataset/PennTreeBank https://raw.githubusercontent.com/BrownFortress/NLU-2024-Labs/main/labs/dataset/PennTreeBank/ptb.train.txt\n",
        "  !pip install wandb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-7VI3nmAWo4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f15679f9-cb9c-495c-8e85-26abf0c79224"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        }
      ],
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "!wandb login --relogin {userdata.get('wandb')}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NVAS70tOH6Wm"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import numpy as np\n",
        "import wandb\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1sKMPGqdIgX_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b50ab06-1f4b-47b7-fdbc-528168a05f67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "if IN_COLAB:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "\n",
        "save_path = \"drive/MyDrive/NLU/models/\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m3_3RfiV2K2C"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5yxSlpYyH6Wm"
      },
      "outputs": [],
      "source": [
        "#@title LM_RNN\n",
        "\n",
        "\n",
        "class LM_RNN(nn.Module):\n",
        "    def __init__(self, emb_size, hidden_size, output_size, pad_index=0, out_dropout=0.1,\n",
        "                 emb_dropout=0.1, n_layers=1):\n",
        "        super(LM_RNN, self).__init__()\n",
        "        # Token ids to vectors, we will better see this in the next lab\n",
        "        self.embedding = nn.Embedding(output_size, emb_size, padding_idx=pad_index)\n",
        "        # Pytorch's RNN layer: https://pytorch.org/docs/stable/generated/torch.nn.RNN.html\n",
        "        self.rnn = nn.RNN(emb_size, hidden_size, n_layers, bidirectional=False, batch_first=True)\n",
        "        self.pad_token = pad_index\n",
        "        # Linear layer to project the hidden layer to our output space\n",
        "        self.output = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, input_sequence):\n",
        "        emb = self.embedding(input_sequence)\n",
        "        rnn_out, _  = self.rnn(emb)\n",
        "        output = self.output(rnn_out).permute(0,2,1)\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Bi1VCcu9DqN"
      },
      "outputs": [],
      "source": [
        "#@title LM_LSTM\n",
        "\n",
        "\n",
        "class LM_LSTM(nn.Module):\n",
        "    def __init__(self, emb_size, hidden_size, output_size, pad_index=0, out_dropout=0.1,\n",
        "                 emb_dropout=0.1, n_layers=1):\n",
        "        super(LM_LSTM, self).__init__()\n",
        "        # Token ids to vectors, we will better see this in the next lab\n",
        "        self.embedding = nn.Embedding(output_size, emb_size, padding_idx=pad_index)\n",
        "        # Pytorch's RNN layer: https://pytorch.org/docs/stable/generated/torch.nn.RNN.html\n",
        "        self.rnn = nn.LSTM(emb_size, hidden_size, n_layers, bidirectional=False,batch_first=True)\n",
        "        self.pad_token = pad_index\n",
        "        # Linear layer to project the hidden layer to our output space\n",
        "        self.output = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "\n",
        "    def forward(self, input_sequence):\n",
        "        emb = self.embedding(input_sequence)\n",
        "        rnn_out, _  = self.rnn(emb)\n",
        "        output = self.output(rnn_out).permute(0,2,1)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qlknOYxxC2dw"
      },
      "outputs": [],
      "source": [
        "#@title LM_LSTM_DROP\n",
        "\n",
        "\n",
        "class LM_LSTM_DROP(nn.Module):\n",
        "    def __init__(self, emb_size, hidden_size, output_size, pad_index=0, out_dropout=0.1,\n",
        "                 emb_dropout=0.1,ins_dropout=0.1, n_layers=1):\n",
        "        super(LM_LSTM_DROP, self).__init__()\n",
        "        # Token ids to vectors, we will better see this in the next lab\n",
        "        self.embedding = nn.Embedding(output_size, emb_size, padding_idx=pad_index)\n",
        "        # Pytorch's RNN layer: https://pytorch.org/docs/stable/generated/torch.nn.RNN.html\n",
        "        self.rnn = nn.LSTM(emb_size, hidden_size, n_layers, dropout=ins_drop, bidirectional=False, batch_first=True)\n",
        "        self.pad_token = pad_index\n",
        "        # Linear layer to project the hidden layer to our output space\n",
        "        self.output = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "        #dropout layers\n",
        "        self.emb_drop = nn.Dropout(p=emb_dropout)\n",
        "        self.out_drop = nn.Dropout(p=out_dropout)\n",
        "\n",
        "    def forward(self, input_sequence):\n",
        "        emb = self.embedding(input_sequence)\n",
        "        drop1 = self.emb_drop(emb)\n",
        "        rnn_out, _  = self.rnn(drop1)\n",
        "        drop2 = self.out_drop(rnn_out)\n",
        "        output = self.output(drop2).permute(0,2,1)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class VarDropout(nn.Module):\n",
        "    def __init__(self, p=0.1):\n",
        "        super(VarDropout, self).__init__()\n",
        "        self.p = p\n",
        "\n",
        "    def forward(self, input_sequence):\n",
        "        rand_mask = torch.rand((input_sequence.shape[::2]),requires_grad=True, device=\"cuda\")\n",
        "        batch_mask = (rand_mask > self.p).int()\n",
        "        expanded_mask = batch_mask.unsqueeze(1)\n",
        "        full_mask = expanded_mask.repeat(1,input_sequence.shape[1],1)\n",
        "        return (input_sequence*full_mask) * (1.0/(1.0-self.p))"
      ],
      "metadata": {
        "id": "FkJ6eoAOM2xn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title LM_LSTM_TWO\n",
        "import pdb\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class LM_LSTM_TWO(nn.Module):\n",
        "    def __init__(self,\n",
        "                 emb_size,\n",
        "                 hidden_size,\n",
        "                 output_size,\n",
        "                 tie = True,\n",
        "                 var_drop = True,\n",
        "                 pad_index=0,\n",
        "                 emb_dropout=0.1,\n",
        "                 out_dropout=0.1,\n",
        "                 n_layers=1):\n",
        "        super(LM_LSTM_TWO, self).__init__()\n",
        "\n",
        "        if tie:\n",
        "          if hidden_size != emb_size:\n",
        "            print(\"WARNING: hidden size and emb size not tied\")\n",
        "          hidden_size = emb_size\n",
        "\n",
        "\n",
        "\n",
        "        self.embedding = nn.Embedding(output_size, emb_size, padding_idx=pad_index)\n",
        "\n",
        "        self.rnn = nn.LSTM(emb_size, hidden_size, n_layers, bidirectional=False, batch_first=True)\n",
        "\n",
        "        self.pad_token = pad_index\n",
        "\n",
        "        # Linear layer to project the hidden layer to our output space\n",
        "        self.decoder = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "        if tie:\n",
        "          self.decoder.weight = self.embedding.weight\n",
        "\n",
        "\n",
        "        if var_drop:\n",
        "          self.drop_emb = VarDropout(p=emb_dropout)\n",
        "          self.drop_dec = VarDropout(p=out_dropout)\n",
        "        else:\n",
        "          self.drop_emb = nn.Dropout(p=emb_dropout)\n",
        "          self.drop_dec = nn.Dropout(p=out_dropout)\n",
        "\n",
        "    def forward(self, input_sequence):\n",
        "        embedded = self.embedding(input_sequence)\n",
        "\n",
        "        dropped_1 = self.drop_emb(embedded)\n",
        "\n",
        "        rnn_out, _  = self.rnn(dropped_1)\n",
        "\n",
        "        dropped_2 = self.drop_dec(rnn_out)\n",
        "\n",
        "        output = self.decoder(dropped_2).permute(0,2,1)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "tX5O59WJ-K4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e75vUM7VH6Wn"
      },
      "outputs": [],
      "source": [
        "DEVICE = 'cuda:0' # it can be changed with 'cpu' if you do not have a gpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "bkrJFqmkH6Wo"
      },
      "outputs": [],
      "source": [
        "#@title Corpus\n",
        "\n",
        "\n",
        "# Loading the corpus\n",
        "\n",
        "def read_file(path, eos_token=\"<eos>\"):\n",
        "    output = []\n",
        "    with open(path, \"r\") as f:\n",
        "        for line in f.readlines():\n",
        "            output.append(line.strip() + \" \" + eos_token)\n",
        "    return output\n",
        "\n",
        "# Vocab with tokens to ids\n",
        "def get_vocab(corpus, special_tokens=[]):\n",
        "    output = {}\n",
        "    i = 0\n",
        "    for st in special_tokens:\n",
        "        output[st] = i\n",
        "        i += 1\n",
        "    for sentence in corpus:\n",
        "        for w in sentence.split():\n",
        "            if w not in output:\n",
        "                output[w] = i\n",
        "                i += 1\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LhfGALaKH6Wo"
      },
      "outputs": [],
      "source": [
        "\n",
        "train_raw = read_file(\"dataset/PennTreeBank/ptb.train.txt\")\n",
        "dev_raw = read_file(\"dataset/PennTreeBank/ptb.valid.txt\")\n",
        "test_raw = read_file(\"dataset/PennTreeBank/ptb.test.txt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S7oggIYZH6Wp"
      },
      "outputs": [],
      "source": [
        "# Vocab is computed only on training set\n",
        "# We add two special tokens end of sentence and padding\n",
        "vocab = get_vocab(train_raw, [\"<pad>\", \"<eos>\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2tnS7iQH6Wp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f21b58c-0c25-4324-8558-aa936faceda6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10001"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "len(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "oFQcOUwtH6Wq"
      },
      "outputs": [],
      "source": [
        "#@title Vocab\n",
        "\n",
        "\n",
        "# This class computes and stores our vocab\n",
        "# Word to ids and ids to word\n",
        "class Lang():\n",
        "    def __init__(self, corpus, special_tokens=[]):\n",
        "        self.word2id = self.get_vocab(corpus, special_tokens)\n",
        "        self.id2word = {v:k for k, v in self.word2id.items()}\n",
        "    def get_vocab(self, corpus, special_tokens=[]):\n",
        "        output = {}\n",
        "        i = 0\n",
        "        for st in special_tokens:\n",
        "            output[st] = i\n",
        "            i += 1\n",
        "        for sentence in corpus:\n",
        "            for w in sentence.split():\n",
        "                if w not in output:\n",
        "                    output[w] = i\n",
        "                    i += 1\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pVxtdll6H6Wr"
      },
      "outputs": [],
      "source": [
        "lang = Lang(train_raw, [\"<pad>\", \"<eos>\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "KhVLmCPOH6Wr"
      },
      "outputs": [],
      "source": [
        "#@title Dataset\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.utils.data as data\n",
        "\n",
        "class PennTreeBank (data.Dataset):\n",
        "    # Mandatory methods are __init__, __len__ and __getitem__\n",
        "    def __init__(self, corpus, lang):\n",
        "        self.source = []\n",
        "        self.target = []\n",
        "\n",
        "        for sentence in corpus:\n",
        "            self.source.append(sentence.split()[0:-1]) # We get from the first token till the second-last token\n",
        "            self.target.append(sentence.split()[1:]) # We get from the second token till the last token\n",
        "            # See example in section 6.2\n",
        "\n",
        "        self.source_ids = self.mapping_seq(self.source, lang)\n",
        "        self.target_ids = self.mapping_seq(self.target, lang)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.source)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src= torch.LongTensor(self.source_ids[idx])\n",
        "        trg = torch.LongTensor(self.target_ids[idx])\n",
        "        sample = {'source': src, 'target': trg}\n",
        "        return sample\n",
        "\n",
        "    # Auxiliary methods\n",
        "\n",
        "    def mapping_seq(self, data, lang): # Map sequences of tokens to corresponding computed in Lang class\n",
        "        res = []\n",
        "        for seq in data:\n",
        "            tmp_seq = []\n",
        "            for x in seq:\n",
        "                if x in lang.word2id:\n",
        "                    tmp_seq.append(lang.word2id[x])\n",
        "                else:\n",
        "                    print('OOV found!')\n",
        "                    print('You have to deal with that') # PennTreeBank doesn't have OOV but \"Trust is good, control is better!\"\n",
        "                    break\n",
        "            res.append(tmp_seq)\n",
        "        return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jjt6QfQ7H6Wr"
      },
      "outputs": [],
      "source": [
        "train_dataset = PennTreeBank(train_raw, lang)\n",
        "dev_dataset = PennTreeBank(dev_raw, lang)\n",
        "test_dataset = PennTreeBank(test_raw, lang)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "62BwUsjGH6Ws",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title DataLoaders\n",
        "\n",
        "from functools import partial\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def collate_fn(data, pad_token):\n",
        "    def merge(sequences):\n",
        "        '''\n",
        "        merge from batch * sent_len to batch * max_len\n",
        "        '''\n",
        "        lengths = [len(seq) for seq in sequences]\n",
        "        max_len = 1 if max(lengths)==0 else max(lengths)\n",
        "        # Pad token is zero in our case\n",
        "        # So we create a matrix full of PAD_TOKEN (i.e. 0) with the shape\n",
        "        # batch_size X maximum length of a sequence\n",
        "        padded_seqs = torch.LongTensor(len(sequences),max_len).fill_(pad_token)\n",
        "        for i, seq in enumerate(sequences):\n",
        "            end = lengths[i]\n",
        "            padded_seqs[i, :end] = seq # We copy each sequence into the matrix\n",
        "        padded_seqs = padded_seqs.detach()  # We remove these tensors from the computational graph\n",
        "        return padded_seqs, lengths\n",
        "\n",
        "    # Sort data by seq lengths\n",
        "\n",
        "    data.sort(key=lambda x: len(x[\"source\"]), reverse=True)\n",
        "    new_item = {}\n",
        "    for key in data[0].keys():\n",
        "        new_item[key] = [d[key] for d in data]\n",
        "\n",
        "    source, _ = merge(new_item[\"source\"])\n",
        "    target, lengths = merge(new_item[\"target\"])\n",
        "\n",
        "    new_item[\"source\"] = source.to(DEVICE)\n",
        "    new_item[\"target\"] = target.to(DEVICE)\n",
        "    new_item[\"number_tokens\"] = sum(lengths)\n",
        "    return new_item\n",
        "\n",
        "# Dataloader instantiation\n",
        "# You can reduce the batch_size if the GPU memory is not enough\n",
        "train_loader = DataLoader(train_dataset, batch_size=256, collate_fn=partial(collate_fn, pad_token=lang.word2id[\"<pad>\"]),  shuffle=True)\n",
        "dev_loader = DataLoader(dev_dataset, batch_size=1024, collate_fn=partial(collate_fn, pad_token=lang.word2id[\"<pad>\"]))\n",
        "test_loader = DataLoader(test_dataset, batch_size=1024, collate_fn=partial(collate_fn, pad_token=lang.word2id[\"<pad>\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GNG5PPaYH6Ws",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Loops and Init\n",
        "\n",
        "import math\n",
        "def train_loop(data, optimizer, criterion, model, clip=5):\n",
        "    model.train()\n",
        "    loss_array = []\n",
        "    number_of_tokens = []\n",
        "\n",
        "    for sample in data:\n",
        "\n",
        "        optimizer.zero_grad() # Zeroing the gradient\n",
        "        output = model(sample['source'])\n",
        "        loss = criterion(output, sample['target'])\n",
        "        loss_array.append(loss.item() * sample[\"number_tokens\"])\n",
        "        number_of_tokens.append(sample[\"number_tokens\"])\n",
        "        loss.backward() # Compute the gradient, deleting the computational graph\n",
        "        # clip the gradient to avoid explosioning gradients\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step() # Update the weights\n",
        "\n",
        "    ppl = math.exp(sum(loss_array) / sum(number_of_tokens))\n",
        "    loss_to_return = sum(loss_array)/sum(number_of_tokens)\n",
        "    return ppl, loss_to_return\n",
        "\n",
        "def eval_loop(data, eval_criterion, model):\n",
        "    model.eval()\n",
        "    loss_to_return = []\n",
        "    loss_array = []\n",
        "    number_of_tokens = []\n",
        "    # softmax = nn.Softmax(dim=1) # Use Softmax if you need the actual probability\n",
        "    with torch.no_grad(): # It used to avoid the creation of computational graph\n",
        "        for sample in data:\n",
        "            output = model(sample['source'])\n",
        "            loss = eval_criterion(output, sample['target'])\n",
        "            loss_array.append(loss.item())\n",
        "            number_of_tokens.append(sample[\"number_tokens\"])\n",
        "\n",
        "    ppl = math.exp(sum(loss_array) / sum(number_of_tokens))\n",
        "    loss_to_return = sum(loss_array) / sum(number_of_tokens)\n",
        "    return ppl, loss_to_return\n",
        "\n",
        "def init_weights(mat):\n",
        "    for m in mat.modules():\n",
        "        if type(m) in [nn.GRU, nn.LSTM, nn.RNN]:\n",
        "            for name, param in m.named_parameters():\n",
        "                if 'weight_ih' in name:\n",
        "                    for idx in range(4):\n",
        "                        mul = param.shape[0]//4\n",
        "                        torch.nn.init.xavier_uniform_(param[idx*mul:(idx+1)*mul])\n",
        "                elif 'weight_hh' in name:\n",
        "                    for idx in range(4):\n",
        "                        mul = param.shape[0]//4\n",
        "                        torch.nn.init.orthogonal_(param[idx*mul:(idx+1)*mul])\n",
        "                elif 'bias' in name:\n",
        "                    param.data.fill_(0)\n",
        "        else:\n",
        "            if type(m) in [nn.Linear]:\n",
        "                torch.nn.init.uniform_(m.weight, -0.01, 0.01)\n",
        "                if m.bias != None:\n",
        "                    m.bias.data.fill_(0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EUavwJBjH6Ws",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "4c25e5bd-9fc0-4d7b-ca68-11ee00e76900"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "LM_LSTM_TWO.__init__() got an unexpected keyword argument 'ins_dropout'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-a9114b4090ed>\u001b[0m in \u001b[0;36m<cell line: 36>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m#                       pad_index=lang.word2id[\"<pad>\"]).to(device)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m model = LM_LSTM_TWO( emb_size,\n\u001b[0m\u001b[1;32m     37\u001b[0m                       \u001b[0mhid_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                       \u001b[0mvocab_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: LM_LSTM_TWO.__init__() got an unexpected keyword argument 'ins_dropout'"
          ]
        }
      ],
      "source": [
        "#@title Single Experiment\n",
        "\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import copy\n",
        "from time import strftime, localtime, time\n",
        "import os\n",
        "\n",
        "emb_size = 200\n",
        "hid_size = 200\n",
        "lr = 0.001\n",
        "clip = 5\n",
        "n_layers = 1\n",
        "emb_drop = 0.5\n",
        "ins_drop = 0.1\n",
        "out_drop = 0.1\n",
        "tying = True\n",
        "var_drop = True\n",
        "\n",
        "device = 'cuda:0'\n",
        "\n",
        "vocab_len = len(lang.word2id)\n",
        "\n",
        "#model = LM_RNN(emb_size, hid_size, vocab_len, pad_index=lang.word2id[\"<pad>\"]).to(device)\n",
        "# model = LM_LSTM(emb_size, hid_size, vocab_len, pad_index=lang.word2id[\"<pad>\"]).to(device)\n",
        "# model = LM_LSTM_DROP( emb_size,\n",
        "#                       hid_size,\n",
        "#                       vocab_len,\n",
        "#                       out_dropout=out_drop,\n",
        "#                       ins_dropout=ins_drop,\n",
        "#                       emb_dropout=emb_drop,\n",
        "#                       n_layers=n_layers,\n",
        "#                       pad_index=lang.word2id[\"<pad>\"]).to(device)\n",
        "\n",
        "model = LM_LSTM_TWO( emb_size,\n",
        "                      hid_size,\n",
        "                      vocab_len,\n",
        "                      tie=tying,\n",
        "                      out_dropout=out_drop,\n",
        "                      ins_dropout=ins_drop,\n",
        "                      emb_dropout=emb_drop,\n",
        "                      n_layers=n_layers,\n",
        "                      pad_index=lang.word2id[\"<pad>\"]).to(device)\n",
        "\n",
        "model.apply(init_weights)\n",
        "\n",
        "#optimizer = optim.SGD(model.parameters(), lr=lr)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
        "\n",
        "criterion_train = nn.CrossEntropyLoss(ignore_index=lang.word2id[\"<pad>\"])\n",
        "criterion_eval = nn.CrossEntropyLoss(ignore_index=lang.word2id[\"<pad>\"], reduction='sum')\n",
        "\n",
        "# build run folder\n",
        "run_name = strftime('run_%Y%m%d_%H%M%S', localtime(time()))\n",
        "run_path = save_path + run_name + \"/\"\n",
        "os.mkdir(run_path)\n",
        "\n",
        "# start a new wandb run to track this script\n",
        "wandb.init(\n",
        "    # set the wandb project where this run will be logged\n",
        "    project=\"NLU_assignment\",\n",
        "    name= run_name,\n",
        "    config={\n",
        "        \"model\":str(type(model).__name__),\n",
        "        \"lr\":lr,\n",
        "        \"optim\":str(type(optimizer).__name__),\n",
        "        \"clip\":clip,\n",
        "        \"hid_size\" : hid_size,\n",
        "        \"emb_size\" : emb_size,\n",
        "        \"layers\":n_layers,\n",
        "        \"tie\":tying,\n",
        "        \"var_drop\":var_drop,\n",
        "        \"dropout\": [emb_drop, out_drop]\n",
        "\n",
        "    }\n",
        ")\n",
        "\n",
        "EPOCHS = 100\n",
        "PAT = 3\n",
        "SAVE_RATE = 1\n",
        "losses_train = []\n",
        "losses_dev = []\n",
        "sampled_epochs = []\n",
        "best_ppl = math.inf\n",
        "best_model = None\n",
        "best_epoch = -1\n",
        "pbar = tqdm(range(1,EPOCHS))\n",
        "\n",
        "\n",
        "for epoch in pbar:\n",
        "    ppl_train, loss = train_loop(train_loader, optimizer, criterion_train, model, clip)\n",
        "\n",
        "    if epoch % 1 == 0:\n",
        "        sampled_epochs.append(epoch)\n",
        "        losses_train.append(np.asarray(loss).mean())\n",
        "\n",
        "        ppl_dev, loss_dev = eval_loop(dev_loader, criterion_eval, model)\n",
        "        losses_dev.append(np.asarray(loss_dev).mean())\n",
        "\n",
        "        if  ppl_dev < best_ppl: # the lower, the better\n",
        "            best_ppl = ppl_dev\n",
        "            best_epoch = epoch\n",
        "            patience = PAT\n",
        "            best_model = copy.deepcopy(model).to('cpu')\n",
        "        else:\n",
        "            patience -= 1\n",
        "\n",
        "        pbar.set_description(\"PPL: \" + str(round(ppl_dev,2)) + \" best: \" + str(round(best_ppl,2))+ \" P: \" + str(patience))\n",
        "        wandb.log({\"ppl\": ppl_dev,\"ppl_train\":ppl_train, \"loss\": loss_dev})\n",
        "\n",
        "    if epoch % SAVE_RATE == 0:\n",
        "      checkpoint_path = run_path + \"epoch_\" +( \"0000\" + str(epoch))[-4:] + \".pt\"\n",
        "      torch.save(model.state_dict(), checkpoint_path)\n",
        "\n",
        "    if patience <= 0: # Early stopping with patience\n",
        "          break # Not nice but it keeps the code clean\n",
        "\n",
        "\n",
        "\n",
        "checkpoint_path = run_path + \"epoch_\" +( \"0000\" + str(epoch))[-4:] + \".pt\"\n",
        "torch.save(model.state_dict(), checkpoint_path)\n",
        "\n",
        "\n",
        "best_model.to(device)\n",
        "final_ppl,  _ = eval_loop(test_loader, criterion_eval, best_model)\n",
        "\n",
        "print('Best ppl: ', best_ppl)\n",
        "print('Test ppl: ', final_ppl)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Multiple Experiments\n",
        "\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import copy\n",
        "from time import strftime, localtime, time\n",
        "import os\n",
        "\n",
        "\n",
        "defaults = { \"emb_size\" : 300,\n",
        "          \"hid_size\" : 300,\n",
        "          \"lr\" : 1.5,\n",
        "          \"clip\" : 5,\n",
        "          \"n_layers\" : 1,\n",
        "          \"emb_drop\" : 0.1,\n",
        "          \"out_drop\" : 0.1,\n",
        "          \"tying\" : False,\n",
        "          \"var_drop\" : False,\n",
        "          \"EPOCHS\": 99,\n",
        "          \"OPT\":\"SGD\"}\n",
        "\n",
        "experiments = [\n",
        "\n",
        "              {\n",
        "                \"lr\":1.8,\n",
        "                \"emb_drop\" : 0.5,\n",
        "                \"out_drop\" : 0.5,\n",
        "                \"var_drop\" : True\n",
        "                },\n",
        "                {\n",
        "                \"lr\":2.1,\n",
        "                \"emb_drop\" : 0.5,\n",
        "                \"out_drop\" : 0.5,\n",
        "                \"var_drop\" : True\n",
        "                },\n",
        "              {\n",
        "                \"emb_drop\" : 0.1,\n",
        "                \"out_drop\" : 0.1,\n",
        "                \"var_drop\" : True\n",
        "                },\n",
        "                            {\n",
        "                \"emb_drop\" : 0.25,\n",
        "                \"out_drop\" : 0.0,\n",
        "                \"var_drop\" : True\n",
        "                },\n",
        "              ]\n",
        "\n",
        "\n",
        "for exp in experiments:\n",
        "\n",
        "  args = defaults | exp\n",
        "\n",
        "  print(args)\n",
        "\n",
        "  emb_size = args[\"emb_size\"]\n",
        "  hid_size = args[\"hid_size\"]\n",
        "  lr = args[\"lr\"]\n",
        "  clip = args[\"clip\"]\n",
        "  n_layers = args[\"n_layers\"]\n",
        "  emb_drop = args[\"emb_drop\"]\n",
        "  out_drop = args[\"out_drop\"]\n",
        "  tying = args[\"tying\"]\n",
        "  var_drop = args[\"var_drop\"]\n",
        "  OPT = args[\"OPT\"]\n",
        "  device = 'cuda:0'\n",
        "\n",
        "  vocab_len = len(lang.word2id)\n",
        "\n",
        "  model = LM_LSTM_TWO( emb_size,\n",
        "                        hid_size,\n",
        "                        vocab_len,\n",
        "                        tie=tying,\n",
        "                        out_dropout=out_drop,\n",
        "                        emb_dropout=emb_drop,\n",
        "                        n_layers=n_layers,\n",
        "                        var_drop=var_drop,\n",
        "                        pad_index=lang.word2id[\"<pad>\"]).to(device)\n",
        "\n",
        "  model.apply(init_weights)\n",
        "\n",
        "  if OPT == \"SGD\":\n",
        "    print(\"using SGD\")\n",
        "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
        "  else:\n",
        "    print(\"using AdamW\")\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
        "\n",
        "  criterion_train = nn.CrossEntropyLoss(ignore_index=lang.word2id[\"<pad>\"])\n",
        "  criterion_eval = nn.CrossEntropyLoss(ignore_index=lang.word2id[\"<pad>\"], reduction='sum')\n",
        "\n",
        "  # build run folder\n",
        "  run_name = strftime('run_%Y%m%d_%H%M%S', localtime(time()))\n",
        "  run_path = save_path + run_name + \"/\"\n",
        "  os.mkdir(run_path)\n",
        "\n",
        "  # start a new wandb run to track this script\n",
        "  wandb.init(\n",
        "      # set the wandb project where this run will be logged\n",
        "      project=\"NLU_assignment\",\n",
        "      name= run_name,\n",
        "      config={\n",
        "          \"model\":str(type(model).__name__),\n",
        "          \"lr\":lr,\n",
        "          \"optim\":str(type(optimizer).__name__),\n",
        "          \"clip\":clip,\n",
        "          \"hid_size\" : hid_size,\n",
        "          \"emb_size\" : emb_size,\n",
        "          \"layers\":n_layers,\n",
        "          \"tie\":tying,\n",
        "          \"var_drop\":var_drop,\n",
        "          \"dropout\": [emb_drop, out_drop]\n",
        "\n",
        "      }\n",
        "  )\n",
        "\n",
        "  EPOCHS = args[\"EPOCHS\"]\n",
        "  PAT = 3\n",
        "  SAVE_RATE = 1\n",
        "  losses_train = []\n",
        "  losses_dev = []\n",
        "  sampled_epochs = []\n",
        "  best_ppl = math.inf\n",
        "  best_model = None\n",
        "  best_epoch = -1\n",
        "  pbar = tqdm(range(1,EPOCHS))\n",
        "\n",
        "\n",
        "  for epoch in pbar:\n",
        "      ppl_train, loss = train_loop(train_loader, optimizer, criterion_train, model, clip)\n",
        "\n",
        "      if epoch % 1 == 0:\n",
        "          sampled_epochs.append(epoch)\n",
        "          losses_train.append(np.asarray(loss).mean())\n",
        "\n",
        "          ppl_dev, loss_dev = eval_loop(dev_loader, criterion_eval, model)\n",
        "          losses_dev.append(np.asarray(loss_dev).mean())\n",
        "\n",
        "          if  ppl_dev < best_ppl: # the lower, the better\n",
        "              best_ppl = ppl_dev\n",
        "              best_epoch = epoch\n",
        "              patience = PAT\n",
        "              best_model = copy.deepcopy(model).to('cpu')\n",
        "          else:\n",
        "              patience -= 1\n",
        "\n",
        "          pbar.set_description(\"PPL: \" + str(round(ppl_dev,2)) + \" best: \" + str(round(best_ppl,2))+ \" P: \" + str(patience))\n",
        "          wandb.log({\"ppl\": ppl_dev,\"ppl_train\":ppl_train, \"loss\": loss_dev})\n",
        "\n",
        "      if epoch % SAVE_RATE == 0:\n",
        "        checkpoint_path = run_path + \"epoch_\" +( \"0000\" + str(epoch))[-4:] + \".pt\"\n",
        "        torch.save(model.state_dict(), checkpoint_path)\n",
        "\n",
        "      if patience <= 0: # Early stopping with patience\n",
        "            break # Not nice but it keeps the code clean\n",
        "\n",
        "\n",
        "\n",
        "  checkpoint_path = run_path + \"epoch_\" +( \"0000\" + str(epoch))[-4:] + \".pt\"\n",
        "  torch.save(model.state_dict(), checkpoint_path)\n",
        "\n",
        "\n",
        "  best_model.to(device)\n",
        "  final_ppl,  _ = eval_loop(test_loader, criterion_eval, best_model)\n",
        "\n",
        "  print('Best ppl: ', best_ppl)\n",
        "  print('Test ppl: ', final_ppl)\n",
        "\n",
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 810,
          "referenced_widgets": [
            "e7e9ff7a052c467cb967f316d212a7da",
            "e91db16f5eea43b2a26f40470c818de9",
            "fcfd370610544ffea1de9d8205b2b466",
            "02a550ebbc664de79ab4aa706588f128",
            "ee4c5a6260354b04844dd3c9991c6117",
            "940bbca54d654f2c9a2b696b97aca7c1",
            "ad9255b9c6c14f969085e0012aa780d1",
            "cb0380cf910e475095c491c1e58ed0c1"
          ]
        },
        "id": "S6TOPydiSj02",
        "outputId": "56991edc-b141-4ba3-d606-857d786a6506"
      },
      "execution_count": 21,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'emb_size': 300, 'hid_size': 300, 'lr': 1.8, 'clip': 5, 'n_layers': 1, 'emb_drop': 0.5, 'out_drop': 0.5, 'tying': False, 'var_drop': True, 'EPOCHS': 99, 'OPT': 'SGD'}\n",
            "using SGD\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Finishing last run (ID:o09fypdm) before initializing another..."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e7e9ff7a052c467cb967f316d212a7da",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>█▆▆▅▅▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>ppl</td><td>█▅▅▄▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>ppl_train</td><td>█▄▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>5.45467</td></tr><tr><td>ppl</td><td>233.8476</td></tr><tr><td>ppl_train</td><td>205.82501</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">run_20240411_171401</strong> at: <a href='https://wandb.ai/gan-gang/NLU_assignment/runs/o09fypdm' target=\"_blank\">https://wandb.ai/gan-gang/NLU_assignment/runs/o09fypdm</a><br/> View project at: <a href='https://wandb.ai/gan-gang/NLU_assignment' target=\"_blank\">https://wandb.ai/gan-gang/NLU_assignment</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240411_171405-o09fypdm/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Successfully finished last run (ID:o09fypdm). Initializing new run:<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.16.6"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240411_173737-1x3zlb79</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/gan-gang/NLU_assignment/runs/1x3zlb79' target=\"_blank\">run_20240411_173737</a></strong> to <a href='https://wandb.ai/gan-gang/NLU_assignment' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/gan-gang/NLU_assignment' target=\"_blank\">https://wandb.ai/gan-gang/NLU_assignment</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/gan-gang/NLU_assignment/runs/1x3zlb79' target=\"_blank\">https://wandb.ai/gan-gang/NLU_assignment/runs/1x3zlb79</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PPL: 211.34 best: 211.34 P: 3:  70%|███████   | 69/98 [34:37<14:33, 30.11s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-23488eaa53d2>\u001b[0m in \u001b[0;36m<cell line: 50>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m       \u001b[0mppl_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-6d33c2edd024>\u001b[0m in \u001b[0;36mtrain_loop\u001b[0;34m(data, optimizer, criterion, model, clip)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mnumber_of_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Zeroing the gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-17-99ca202f6bb5>\u001b[0m in \u001b[0;36mcollate_fn\u001b[0;34m(data, pad_token)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_item\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"target\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mnew_item\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"source\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mnew_item\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"target\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mnew_item\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"number_tokens\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0A7h-fSvH6Wt"
      },
      "outputs": [],
      "source": [
        "\n",
        "# To load the model you need to initialize it\n",
        "print('Best ppl: ', best_ppl)\n",
        "test_ep = best_epoch\n",
        "\n",
        "if test_ep == -1:\n",
        "  paths = os.listdir(run_path)\n",
        "  paths.sort()\n",
        "  checkpoint_path = run_path + paths[-1]\n",
        "else:\n",
        "  checkpoint_path = run_path + \"epoch_\" +( \"0000\" + str(test_ep))[-4:] + \".pt\"\n",
        "\n",
        "model.load_state_dict(torch.load(checkpoint_path))\n",
        "model.to(DEVICE)\n",
        "final_ppl,  _ = eval_loop(test_loader, criterion_eval, model)\n",
        "print('Test ppl: ', final_ppl)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWXTuavOH6Wt"
      },
      "source": [
        "# Mandatory Exam Exercise\n",
        "## Part 1 (4 points)\n",
        "In this, you have to modify the baseline LM_RNN by adding a set of techniques that might improve the performance. In this, you have to add one modification at a time incrementally. If adding a modification decreases the performance, you can remove it and move forward with the others. However, in the report, you have to provide and comment on this unsuccessful experiment.  For each of your experiments, you have to print the performance expressed with Perplexity (PPL).\n",
        "<br>\n",
        "One of the important tasks of training a neural network is  hyperparameter optimization. Thus, you have to play with the hyperparameters to minimise the PPL and thus print the results achieved with the best configuration (in particular <b>the learning rate</b>).\n",
        "These are two links to the state-of-the-art papers which use vanilla RNN [paper1](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5947611), [paper2](https://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf).\n",
        "\n",
        "**Mandatory requirements**: For the following experiments the perplexity must be below 250 (***PPL < 250***).\n",
        "\n",
        "1. Replace RNN with a Long-Short Term Memory (LSTM) network --> [link](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html)\n",
        "2. Add two dropout layers: --> [link](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html)\n",
        "    - one after the embedding layer,\n",
        "    - one before the last linear layer\n",
        "3. Replace SGD with AdamW --> [link](https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "veiO8_AkH6Wt"
      },
      "source": [
        "## Part 2 (11 points)\n",
        "**Mandatory requirements**: For the following experiments the perplexity must be below 250 (***PPL < 250***) and it should be lower than the one achieved in Part 1.1 (i.e. base LSTM).\n",
        "\n",
        "Starting from the `LM_RNN` in which you replaced the RNN with a LSTM model, apply the following regularisation techniques:\n",
        "- Weight Tying\n",
        "- Variational Dropout (no DropConnect)\n",
        "- Non-monotonically Triggered AvSGD\n",
        "\n",
        "These techniques are described in [this paper](https://openreview.net/pdf?id=SyyGPP0TZ).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hH1SvTPH6Wt"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e7e9ff7a052c467cb967f316d212a7da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e91db16f5eea43b2a26f40470c818de9",
              "IPY_MODEL_fcfd370610544ffea1de9d8205b2b466"
            ],
            "layout": "IPY_MODEL_02a550ebbc664de79ab4aa706588f128"
          }
        },
        "e91db16f5eea43b2a26f40470c818de9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ee4c5a6260354b04844dd3c9991c6117",
            "placeholder": "​",
            "style": "IPY_MODEL_940bbca54d654f2c9a2b696b97aca7c1",
            "value": "0.011 MB of 0.011 MB uploaded\r"
          }
        },
        "fcfd370610544ffea1de9d8205b2b466": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ad9255b9c6c14f969085e0012aa780d1",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cb0380cf910e475095c491c1e58ed0c1",
            "value": 1
          }
        },
        "02a550ebbc664de79ab4aa706588f128": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee4c5a6260354b04844dd3c9991c6117": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "940bbca54d654f2c9a2b696b97aca7c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ad9255b9c6c14f969085e0012aa780d1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb0380cf910e475095c491c1e58ed0c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}